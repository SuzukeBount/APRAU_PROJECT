Reinforcement Learning é um tipo de machine learning onde o agente aprende através de modelos, criando decisões ao interagir com o ambiente apresentado . Ao contrário da aprendizagem não supervisionada , RL funciona de maneira que o agente pode aprender com as consequências de suas ações , por meio de exploração , recebendo um feedback em forma de recompensa ou penalidade .

Agente: É o responsável por aprender e escolher . Por exemplo, pode ser um jogador que faz movimentos ou interage com o ambiente .
Ambiente : Um espaço em um determinado mundo onde o agente opera, contendo estados, ações, recompensas, penalidades, etc .
Estado (S): Representação do ambiente em um determinado ponto .
Ação (A): São as possibilidades de escolha que um agente pode ter em determinados estados do ambiente .
Recompensa (R): É algo que o agente recebe após completar uma ação, podendo ser esta positiva ou negativa .
Política (π): Estratégia utilizada por um agente para escolher uma ação .
Função de Valor (V): Uma função que prediz a recompensa acumulativa de um agente .
Q- Valor (Q): Similar à função anterior, mas avalia o valor de tomar uma certa ação em um determinado ponto .
Exploração vs Exploração : Exploração refere -se ao fato de que o agente é livre para encontrar um caminho que seja o mais adequado para ele, enquanto que exploração refere -se a buscar a maior quantidade de recompensas possíveis, mesmo quando o risco é maior.

O processo de Reinforcement Learning pode ser dividido em 4 etapas fundamentais :
Inicialização: O agente começa em um estado inicial, tendo um valor neutro , objetivo neutro ou randomizado .
Interação: Tudo isso envolve fazer ações no mundo, interagir com o ambiente, movimentar-se nele, etc .
Aprendizagem: O agente irá aprender após cada ação, podendo ser esta positiva ou negativa, dependendo de como o agente interpretará esses valores .
Terminação: É o estado final do agente, quando a condição de término é alcançada, podendo ser completar o objetivo ou , porventura , não ter conseguido finalizar o objetivo no tempo estabelecido.

O algoritmo que foi escolhido e utilizado em nosso dataset foi o Q-learning . Devido à exploração dos algoritmos e comparação com o dataset, foi determinado que este seria o mais adequado.
Funcionamento no nosso dataset:

1. Classe de seleção de características
- Existe uma classe de escolha de características, onde são definidas pelo próprio dataset .
- Os estados estão em arrays binários, representando se uma característica foi escolhida .
- As ações envolvem a seleção de características, podendo ser positivas ou negativas para o agente .
- As recompensas e o tipo de recompensa estão diretamente ligadas ao accuracy do modelo utilizado .

2. Classe de aprendizagem do agente
- Classe que irá implementar o agente de Q-learning .
- O agente irá aprender uma Q-table que decidirá quais características serão escolhidas .
- Utiliza ε-greedy para exercícios de exploração versus exploração.

3. Loop utilizado para o treinamento
- Treina o agente um número de episódios escolhidos previamente .
- Atualiza automaticamente o Q-Value (variável objetivo) dependendo das ações tomadas e das recompensas atribuídas.

Ainda assim , teve que ser escolhido 2 modelos para a implementação dos algoritmos de Q-learning .
Estes dois modelos foram:
- Random Forest completo com todos os atributos .
- Support Vector Machine Classifier com kernel RBF e todos os atributos .

Ao longo do dataset, estes dois modelos foram os mais corretos .
Os resultados graficamente podem ser vistos no PNG chamado "R_learning.png" .

Analisando o gráfico :
- Na maioria dos episódios e ações que o agente completa , a recompensa costuma ser algo positivo.
- O Q-Value atinge muitas vezes o valor máximo de 50, sugerindo que o agente está aprendendo progressivamente, selecionando características mais ótimas.
- Existem ainda quedas significativas, devido à exploração , sendo que quando o agente é "greedy" , ele perde uma certa performance .

Em conclusão,
com o trabalho realizado no dataset, nos modelos e na análise dos resultados, podemos concluir que o agente tem um grande potencial de aprendizagem , apresentando uma alta percentagem de acertos e não de erros .